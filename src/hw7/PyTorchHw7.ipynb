{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from tqdm import tqdm\n",
        "import nltk"
      ],
      "metadata": {
        "id": "pJruLsa-qZpj"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "CUDin2qbqbpO"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yqjei5iq5MV",
        "outputId": "948045fa-f6c8-46f4-c599-03390fbb1d34"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка текста\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "CTGP9VO3qd18"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Привести к нижнему регистру\n",
        "    tokens = word_tokenize(text)  # Токенизация\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Убрать пунктуацию и цифры\n",
        "    tokens = [token for token in tokens if token not in stop_words]  # Убрать стоп-слова\n",
        "    tokens = [stemmer.stem(token) for token in tokens]  # Стемминг\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "hreevqX1qgKN"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['processed_tweet'] = train_data['tweet'].apply(preprocess_text)\n",
        "test_data['processed_tweet'] = test_data['tweet'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "xrIKeBJ8q-w3"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря слов\n",
        "all_words = [word for tokens in train_data['processed_tweet'] for word in tokens]\n",
        "all_words.extend([word for tokens in test_data['processed_tweet'] for word in tokens])  # Включаем слова из тестовых данных\n",
        "word_counter = Counter(all_words)\n",
        "vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n",
        "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}  # +1 для резервирования 0 для паддинга\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}"
      ],
      "metadata": {
        "id": "MOTuukPKrAk_"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование текста в последовательности индексов\n",
        "def encode_text(text):\n",
        "    return [word_to_idx.get(word, 0) for word in text]  # Используем get для неизвестных слов"
      ],
      "metadata": {
        "id": "yvIt6kDHrD-f"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['encoded_tweet'] = train_data['processed_tweet'].apply(encode_text)\n",
        "test_data['encoded_tweet'] = test_data['processed_tweet'].apply(encode_text)"
      ],
      "metadata": {
        "id": "NKj9rdjorHE-"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание датасета\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "HT-wZgRCrIdc"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(train_data['encoded_tweet'])\n",
        "y = list(train_data['label'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "PprTr59Srtvl"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "qtxEWa-Orvjs"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "MOUxfrgbrx1G"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение GRU модели\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        last_hidden = output[:, -1, :]\n",
        "        return self.fc(last_hidden)"
      ],
      "metadata": {
        "id": "FIFJRYXqr0Cy"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация модели и других параметров\n",
        "vocab_size = len(word_to_idx) + 1  # +1 для паддинга\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 2  # Два класса: 0 и 1"
      ],
      "metadata": {
        "id": "skM__HAEr1-0"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ao1P1oE8r5c9"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for inputs, labels in tqdm(data_loader, desc=\"Training\"):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "Kd3iEtBXr-2n"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_preds += (preds == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss / len(data_loader), accuracy"
      ],
      "metadata": {
        "id": "JMlqnwf2sCy3"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "best_val_loss = float('inf')"
      ],
      "metadata": {
        "id": "Dlmdzen0sEyT"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для добавления паддинга и создания тензоров\n",
        "def collate_fn(batch):\n",
        "    data, labels = zip(*batch)\n",
        "    lengths = [len(seq) for seq in data]\n",
        "    max_length = max(lengths)\n",
        "\n",
        "    padded_data = [seq + [0] * (max_length - len(seq)) for seq in data]\n",
        "\n",
        "    return torch.tensor(padded_data), torch.tensor(labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "KVj_7Lkcsurk"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZDy5XFOsHXm",
        "outputId": "6b9c99ea-de87-4f49-bf32-fdcbbe062fe1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:51<00:00,  7.77it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 63.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: Train Loss: 0.2061 | Val Loss: 0.1587 | Val Accuracy: 0.9457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:52<00:00,  7.63it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 63.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10: Train Loss: 0.1249 | Val Loss: 0.1398 | Val Accuracy: 0.9526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:47<00:00,  8.47it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 62.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10: Train Loss: 0.0811 | Val Loss: 0.1461 | Val Accuracy: 0.9548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:46<00:00,  8.51it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 63.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10: Train Loss: 0.0443 | Val Loss: 0.1927 | Val Accuracy: 0.9476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:45<00:00,  8.70it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:02<00:00, 38.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10: Train Loss: 0.0214 | Val Loss: 0.2075 | Val Accuracy: 0.9523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:45<00:00,  8.76it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 62.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10: Train Loss: 0.0114 | Val Loss: 0.2476 | Val Accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:48<00:00,  8.33it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 64.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10: Train Loss: 0.0079 | Val Loss: 0.2696 | Val Accuracy: 0.9529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:47<00:00,  8.44it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 64.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10: Train Loss: 0.0048 | Val Loss: 0.2972 | Val Accuracy: 0.9387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:46<00:00,  8.58it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:02<00:00, 46.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10: Train Loss: 0.0060 | Val Loss: 0.2701 | Val Accuracy: 0.9510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [00:45<00:00,  8.76it/s]\n",
            "Evaluating: 100%|██████████| 100/100 [00:01<00:00, 62.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10: Train Loss: 0.0028 | Val Loss: 0.2849 | Val Accuracy: 0.9481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка лучшей модели и предсказание на тестовых данных\n",
        "best_model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "best_model.load_state_dict(torch.load('best_model.pth'))\n",
        "best_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH7aDugwsLar",
        "outputId": "0ada707b-4002-4f2c-b28d-1f55a6f31d80"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRUModel(\n",
              "  (embedding): Embedding(38660, 100)\n",
              "  (gru): GRU(100, 256, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(list(test_data['encoded_tweet']), [0] * len(test_data))  # Фейковые метки, не используются\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)  # Используем collate_fn\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        outputs = best_model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predictions.extend(preds.cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xf6rMM2s7Io",
        "outputId": "21dac056-30c8-403c-f124-657dcbf8d731"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 269/269 [00:06<00:00, 41.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение предсказаний в файл\n",
        "test_data['label'] = predictions\n",
        "test_data[['id', 'label']].to_csv('predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "gzRzZRePtDt-"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Исходя из результатов, можно сделать следующие выводы:\n",
        "1. Обучение:\n",
        "  - Значение функции потерь (Train Loss) снижается с каждой эпохой, что говорит о том, что модель учится на обучающих данных.\n",
        "  - Однако значение функции потерь на валидационных данных (Val Loss) начинает возрастать после нескольких эпох. Это может указывать на переобучение модели.\n",
        "  - Точность на валидационных данных (Val Accuracy) держится на довольно высоком уровне (в среднем около 95%), что говорит о том, что модель способна хорошо обобщить на новые данные.\n",
        "2. Предсказания:\n",
        "  - Лучшая модель, сохраненная в файле 'best_model.pth', имеет следующую архитектуру: Embedding - GRU - Linear. Embedding имеет размер словаря 38660 и размерность векторов 100.\n",
        "  - В процессе предсказания на тестовых данных модель демонстрирует хорошую скорость (269 батчей обработаны за 6 секунд)."
      ],
      "metadata": {
        "id": "HsuthTMxwJMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Помогло улучшить точность нейронной сети:\n",
        "1. Предобработка текста:\n",
        "  - Приведение текста к нижнему регистру.\n",
        "  - Токенизация текста.\n",
        "  - Удаление пунктуации и цифр.\n",
        "  - Удаление стоп-слов.\n",
        "  - Применение стемминга.\n",
        "2. Включение слов из обучающего и тестового набора данных в словарь.\n",
        "3. Добавление паддинга к коротким твитам для создания батчей с равной длиной.\n",
        "4. Создание GRU-модели с Embedding, GRU-слоем и Linear-слоем.\n",
        "5. Оптимизация и функция потерь:\n",
        "  - Использование оптимизатора Adam.\n",
        "  - Использование функции потерь CrossEntropyLoss.\n",
        "6. Использование DataLoader с collate_fn для обработки разных длин твитов."
      ],
      "metadata": {
        "id": "mEov0pOLxFfg"
      }
    }
  ]
}